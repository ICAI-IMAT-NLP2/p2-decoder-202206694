{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratory Practice 2: Generating names with a decoder-only transformer\n",
    "\n",
    "In this notebook, we will generate new names using a Transformer decoder model. This simple text generation task captures the essential components of language modeling, applied to a small, manageable dataset.\n",
    "\n",
    "More specifically, you will be training autoregressive, character-level, decoder-only language model. You will feed it a database of names, and the model will generate new name ideas that all sound name-like, but are not already existing names. \n",
    "\n",
    "First, you'll train the model. After training, you'll generate names using pure random sampling as your decoding strategy. Pure random sampling doesn't always work well, so you'll also learn to tweak the temperature parameter when sampling, to better control your generation output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import the data processing and model modules\n",
    "from data_processing import load_and_preprocess_data, CharTokenizer, NameDataset, collate_fn\n",
    "from train import train \n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cpu\")# torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertviz==1.4.0 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1))\n",
      "  Downloading bertviz-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting black==23.11.0 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 2))\n",
      "  Using cached black-23.11.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (66 kB)\n",
      "Collecting matplotlib==3.8.2 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3))\n",
      "  Using cached matplotlib-3.8.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting mypy==1.7.0 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 4))\n",
      "  Using cached mypy-1.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (1.8 kB)\n",
      "Collecting numpy==1.26.2 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 5))\n",
      "  Using cached numpy-1.26.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting pandas==2.1.3 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 6))\n",
      "  Using cached pandas-2.1.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting scikit-learn==1.3.2 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 7))\n",
      "  Using cached scikit_learn-1.3.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting tensorboard==2.15.1 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Using cached tensorboard-2.15.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting torch==2.1.1 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 9))\n",
      "  Using cached torch-2.1.1-cp39-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Collecting torchvision==0.16.1 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 10))\n",
      "  Using cached torchvision-0.16.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm==4.66.1 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 11))\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting transformers==4.39.3 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 12))\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting pytest==7.4.4 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 13))\n",
      "  Using cached pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting pytest-order==1.2.0 (from -r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 14))\n",
      "  Using cached pytest_order-1.2.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting boto3 (from bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1))\n",
      "  Downloading boto3-1.40.36-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: requests in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1)) (2.32.3)\n",
      "Collecting regex (from bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1))\n",
      "  Downloading regex-2025.9.18-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting sentencepiece (from bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1))\n",
      "  Downloading sentencepiece-0.2.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from black==23.11.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 2)) (8.1.7)\n",
      "Collecting mypy-extensions>=0.4.3 (from black==23.11.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 2))\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from black==23.11.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 2)) (24.1)\n",
      "Collecting pathspec>=0.9.0 (from black==23.11.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 2))\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from black==23.11.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 2)) (4.3.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from black==23.11.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from black==23.11.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 2)) (4.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from pandas==2.1.3->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 6)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from pandas==2.1.3->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 6)) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from scikit-learn==1.3.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 7)) (1.13.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn==1.3.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 7))\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.3.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 7))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (1.67.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (2.36.0)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting protobuf<4.24,>=3.19.6 (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (58.1.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: filelock in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from torch==2.1.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 9)) (3.16.1)\n",
      "Collecting sympy (from torch==2.1.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 9))\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from torch==2.1.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 9)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from torch==2.1.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 9)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from torch==2.1.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 9)) (2024.10.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.39.3->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 12))\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from transformers==4.39.3->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 12)) (6.0.2)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 12))\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.39.3->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 12))\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting iniconfig (from pytest==7.4.4->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 13))\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pluggy<2.0,>=0.12 (from pytest==7.4.4->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 13))\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from pytest==7.4.4->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 13)) (1.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (4.9)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.3->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 12))\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib==3.8.2->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 3)) (3.20.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from requests->bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from requests->bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from requests->bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from requests->bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (3.0.0)\n",
      "Collecting botocore<1.41.0,>=1.40.36 (from boto3->bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1))\n",
      "  Downloading botocore-1.40.36-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1))\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1))\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.1.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 9))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->bertviz==1.4.0->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 1))\n",
      "  Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/patriciadiaz/vision/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8)) (0.6.1)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard==2.15.1->-r /Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt (line 8))\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n",
      "Using cached black-23.11.0-cp39-cp39-macosx_11_0_arm64.whl (1.4 MB)\n",
      "Using cached matplotlib-3.8.2-cp39-cp39-macosx_11_0_arm64.whl (7.5 MB)\n",
      "Using cached mypy-1.7.0-cp39-cp39-macosx_11_0_arm64.whl (9.9 MB)\n",
      "Using cached numpy-1.26.2-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached pandas-2.1.3-cp39-cp39-macosx_11_0_arm64.whl (11.0 MB)\n",
      "Using cached scikit_learn-1.3.2-cp39-cp39-macosx_12_0_arm64.whl (9.5 MB)\n",
      "Using cached tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "Using cached torch-2.1.1-cp39-none-macosx_11_0_arm64.whl (59.6 MB)\n",
      "Using cached torchvision-0.16.1-cp39-cp39-macosx_11_0_arm64.whl (1.5 MB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytest-7.4.4-py3-none-any.whl (325 kB)\n",
      "Using cached pytest_order-1.2.0-py3-none-any.whl (14 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading google_auth_oauthlib-1.2.2-py3-none-any.whl (19 kB)\n",
      "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Using cached protobuf-4.23.4-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "Downloading regex-2025.9.18-cp39-cp39-macosx_11_0_arm64.whl (286 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.15.2-cp39-cp39-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading boto3-1.40.36-py3-none-any.whl (139 kB)\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading sentencepiece-0.2.1-cp39-cp39-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.40.36-py3-none-any.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Using cached urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Installing collected packages: mpmath, werkzeug, urllib3, tqdm, threadpoolctl, tensorboard-data-server, sympy, sentencepiece, safetensors, regex, protobuf, pluggy, pathspec, oauthlib, numpy, mypy-extensions, joblib, jmespath, iniconfig, hf-xet, absl-py, torch, pytest, pandas, mypy, markdown, botocore, black, torchvision, scikit-learn, s3transfer, requests-oauthlib, pytest-order, matplotlib, huggingface-hub, tokenizers, google-auth-oauthlib, boto3, transformers, tensorboard, bertviz\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.3\n",
      "    Uninstalling urllib3-2.2.3:\n",
      "      Successfully uninstalled urllib3-2.2.3\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.3\n",
      "    Uninstalling pandas-2.2.3:\n",
      "      Successfully uninstalled pandas-2.2.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.9.2\n",
      "    Uninstalling matplotlib-3.9.2:\n",
      "      Successfully uninstalled matplotlib-3.9.2\n",
      "Successfully installed absl-py-2.3.1 bertviz-1.4.0 black-23.11.0 boto3-1.40.36 botocore-1.40.36 google-auth-oauthlib-1.2.2 hf-xet-1.1.10 huggingface-hub-0.35.1 iniconfig-2.1.0 jmespath-1.0.1 joblib-1.5.2 markdown-3.9 matplotlib-3.8.2 mpmath-1.3.0 mypy-1.7.0 mypy-extensions-1.1.0 numpy-1.26.2 oauthlib-3.3.1 pandas-2.1.3 pathspec-0.12.1 pluggy-1.6.0 protobuf-4.23.4 pytest-7.4.4 pytest-order-1.2.0 regex-2025.9.18 requests-oauthlib-2.0.0 s3transfer-0.14.0 safetensors-0.6.2 scikit-learn-1.3.2 sentencepiece-0.2.1 sympy-1.14.0 tensorboard-2.15.1 tensorboard-data-server-0.7.2 threadpoolctl-3.6.0 tokenizers-0.15.2 torch-2.1.1 torchvision-0.16.1 tqdm-4.66.1 transformers-4.39.3 urllib3-1.26.20 werkzeug-3.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r \"/Users/patriciadiaz/Documents/4/p2/p2-decoder-202206694/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "In this section, you will take a look at the data. You should be familiar with it, but it is used a little different now that we are training a decoder model. Play with it and answer the questions at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "First 10 names: ['-maria carmen.', '-antonio.', '-maria.', '-manuel.', '-jose.']\n"
     ]
    }
   ],
   "source": [
    "data_filepath = \"../data/nombres_raw.txt\"  # Replace with your actual file path\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz \"\n",
    "start_token = \"-\"\n",
    "end_token = \".\"\n",
    "batch_size = 64\n",
    "\n",
    "# Load and preprocess data\n",
    "print(\"Loading and preprocessing data...\")\n",
    "names = load_and_preprocess_data(data_filepath, alphabet, start_token, end_token)\n",
    "print(\"First 10 names:\", names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding names...\n",
      "First 10 encoded names: [[1, 16, 4, 21, 12, 4, 3, 6, 4, 21, 16, 8, 17, 2], [1, 4, 17, 23, 18, 17, 12, 18, 2], [1, 16, 4, 21, 12, 4, 2], [1, 16, 4, 17, 24, 8, 15, 2], [1, 13, 18, 22, 8, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = CharTokenizer(alphabet, start_token=start_token, end_token=end_token)\n",
    "\n",
    "# Encode names\n",
    "print(\"Encoding names...\")\n",
    "encoded_names = [tokenizer.encode(name) for name in names]\n",
    "print(\"First 10 encoded names:\", encoded_names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre: patricia\n",
      "Codificado: [1, 19, 4, 23, 21, 12, 6, 12, 4, 2]\n",
      "Decodificado: -patricia.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "name = \"patricia\"\n",
    "text = f\"-{name}.\"\n",
    "encoded = tokenizer.encode(text)\n",
    "\n",
    "print(\"Nombre:\", name)\n",
    "print(\"Codificado:\", encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the ```CharTokenizer``` to encode your name. What is the result of encoding your name?\n",
    ">>> Write your name\n",
    "\n",
    "patricia\n",
    ">>> Write your name encoded\n",
    "\n",
    "[1, 19, 4, 23, 21, 12, 6, 12, 4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First item in batch:\n",
      "Input: tensor([ 1, 25,  4, 22, 12, 15,  8,  3, 10, 11,  8, 18, 21, 10, 11,  8,  0,  0,\n",
      "         0])\n",
      "Target: tensor([25,  4, 22, 12, 15,  8,  3, 10, 11,  8, 18, 21, 10, 11,  8,  2,  0,  0,\n",
      "         0])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = NameDataset(encoded_names)\n",
    "\n",
    "# Create data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Obtain a batch from data loader\n",
    "batch = next(iter(data_loader))\n",
    "\n",
    "# Let´s check one item from batch\n",
    "print(\"First item in batch:\")\n",
    "print(\"Input:\", batch[0][0])\n",
    "print(\"Target:\", batch[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can you obtain the target tensor from the input? Does this make sense for an autorregressive prediction such as the one of the Decoder-only model?\n",
    ">>> Write your answer here\n",
    "Se puede desplazar la posición del tensor de entrada uno hacia la izquierda y reemplazar el último valor por final. Esto tiene sentido en prediccion autorregresiva porque cada token se predice a partir los anteriores.\n",
    "\n",
    "- What is the tensor value for the start token? And for the end token? And for the padding token?\n",
    ">>> Write your answer here\n",
    "El token empieza en la posicion 1, y la posición 0 es padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model and tokenizer\n",
    "Here you will train the decoder model. Feel free to change the hyperparameters of the model in the ```model_params``` dictionary. Be careful with your computational resources!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Starting training...\n",
      "Epoch [1/10], Training Loss: 2.3957\n",
      "Epoch [1/10], Validation Loss: 2.1512\n",
      "Epoch [2/10], Training Loss: 2.0662\n",
      "Epoch [2/10], Validation Loss: 1.9877\n",
      "Epoch [3/10], Training Loss: 1.9202\n",
      "Epoch [3/10], Validation Loss: 1.8640\n",
      "Epoch [4/10], Training Loss: 1.8045\n",
      "Epoch [4/10], Validation Loss: 1.7699\n",
      "Epoch [5/10], Training Loss: 1.7160\n",
      "Epoch [5/10], Validation Loss: 1.6972\n",
      "Epoch [6/10], Training Loss: 1.6484\n",
      "Epoch [6/10], Validation Loss: 1.6375\n",
      "Epoch [7/10], Training Loss: 1.5946\n",
      "Epoch [7/10], Validation Loss: 1.5943\n",
      "Epoch [8/10], Training Loss: 1.5518\n",
      "Epoch [8/10], Validation Loss: 1.5619\n",
      "Epoch [9/10], Training Loss: 1.5172\n",
      "Epoch [9/10], Validation Loss: 1.5281\n",
      "Epoch [10/10], Training Loss: 1.4876\n",
      "Epoch [10/10], Validation Loss: 1.5076\n"
     ]
    }
   ],
   "source": [
    "# Define training hyper parameters\n",
    "model_save_dir = \"runs\"\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "model_params = {\n",
    "    \"d_model\": 64,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"intermediate_size\": 128,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"max_position_embeddings\": tokenizer.vocab_size # Do not touch this\n",
    "}\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Call the train function\n",
    "model = train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    model_save_dir=model_save_dir,\n",
    "    model_params=model_params,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Names\n",
    "Now we are ready to generate new names. Fill the function below and start playing around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prefix: str = \"\",\n",
    "    start_token: str = \"-\",\n",
    "    end_token: str = \".\",\n",
    "    max_length: int = 20,\n",
    "    temperature: float = 1.0,\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a new name using the trained model, optionally starting with a given prefix.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        tokenizer (CharTokenizer): The tokenizer.\n",
    "        prefix (str): Optional prefix string to start the name.\n",
    "        start_token (str): The start token character.\n",
    "        end_token (str): The end token character.\n",
    "        max_length (int): Maximum length of the generated name (excluding prefix length).\n",
    "        temperature (float): Sampling temperature. Higher values increase randomness.\n",
    "        device (torch.device): Device to perform computation on.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated name.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    start_token_id = tokenizer.char2idx[start_token]\n",
    "    end_token_id = tokenizer.char2idx[end_token]\n",
    "\n",
    "    # TODO: Encode the prefix\n",
    "    prefix_ids = tokenizer.encode(f\"{start_token}{prefix}{end_token}\")\n",
    "    prefix_ids = torch.tensor(prefix_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # TODO: Initialize the input with the start token and the prefix\n",
    "    generated_ids = prefix_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # TODO: Get model predictions\n",
    "            logits = model(generated_ids)[:, -1, :] \n",
    "            \n",
    "            # TODO: Apply softmax to get probabilities\n",
    "            next_token_probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "            # TODO: Sample the next token\n",
    "            next_token_id = torch.multinomial(next_token_probs, num_samples=1)\n",
    "            # TODO: Append the new token to the sequence\n",
    "            generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
    "\n",
    "\n",
    "            # Stop if end token is generated\n",
    "            if next_token_id.item() == end_token_id:\n",
    "                break\n",
    "\n",
    "    # TODO: Decode the generated token IDs to a string, excluding start and end tokens\n",
    "    generated_sequence = tokenizer.decode(generated_ids.squeeze().cpu().numpy().tolist())\n",
    "\n",
    "    # TODO: Decode the name\n",
    "    generated_name = generated_sequence[1:-1]\n",
    "\n",
    "    return generated_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_names = 5          # Number of names to generate\n",
    "max_length = 20         # Maximum length of each generated name\n",
    "temperature = 1.0       # Sampling temperature \n",
    "start_token = \"-\"       # Start token character (used during training)\n",
    "end_token = \".\"         # End token character (used during training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate names\n",
    "print(\"Generated Names:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Temperature Parameter in Language Generation\n",
    "\n",
    "The **temperature** parameter $T$ adjusts the randomness of text generated by language models by scaling the logits (model outputs before softmax).\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "Given logits $z_i$ for each token $i$, the probability $p_i$ of selecting token $i$ is calculated using the softmax function:\n",
    "\n",
    "\\begin{align*}\n",
    "p_i = \\frac{\\exp\\left(\\frac{z_i}{T}\\right)}{\\sum_{j} \\exp\\left(\\frac{z_j}{T}\\right)}\n",
    "\\end{align*}\n",
    "\n",
    "- *When $T = 1$*: The probabilities remain unchanged.\n",
    "- *When $T < 1$*: The distribution becomes sharper; higher-probability tokens are favored. You should expect names to look more like the \"typical\" names encountered in the dataset.\n",
    "- *When $T > 1$*: The distribution flattens; lower-probability tokens are more likely. You should expect names to look more \"exotic\" or \"creative\", since less probable characters are being sampled to continue the previously generated ones.\n",
    "\n",
    "### Impact on Token Probabilities\n",
    "\n",
    "Suppose we have logits for three tokens:\n",
    "\n",
    "- $z_A$ = 2.0\n",
    "- $z_B$ = 1.0\n",
    "- $z_C$ = 0.5\n",
    "\n",
    "##### At $T = 1.0$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.659\\\\\n",
    "p_B &= \\frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.242\\\\\n",
    "p_C &= \\frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5}} \\approx 0.099\n",
    "\\end{align*}\n",
    "\n",
    "##### At $T = 0.5$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0 / 0.5}}{e^{2.0 / 0.5} + e^{1.0 / 0.5} + e^{0.5 / 0.5}} = \\frac{e^{4.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.843\\\\\n",
    "p_B &= \\frac{e^{2.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.114\\\\\n",
    "p_C &= \\frac{e^{1.0}}{e^{4.0} + e^{2.0} + e^{1.0}} \\approx 0.043\n",
    "\\end{align*}\n",
    "\n",
    "- **Observation**: Lower $T$ increases the dominance of the highest logit.\n",
    "\n",
    "##### At $T = 1.5$:\n",
    "\n",
    "\\begin{align*}\n",
    "p_A &= \\frac{e^{2.0 / 1.5}}{e^{2.0 / 1.5} + e^{1.0 / 1.5} + e^{0.5 / 1.5}} = \\frac{e^{1.333}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.490\\\\\n",
    "p_B &= \\frac{e^{0.667}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.282\\\\\n",
    "p_C &= \\frac{e^{0.333}}{e^{1.333} + e^{0.667} + e^{0.333}} \\approx 0.228\n",
    "\\end{align*}\n",
    "\n",
    "- **Observation**: Higher $T$ increases the probabilities of less likely tokens.\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "- **Low Temperature ($T < 1$)**:\n",
    "  - **Sharper Distribution**: Model is confident; outputs are more predictable.\n",
    "  - **Use Case**: When coherence is crucial.\n",
    "\n",
    "- **High Temperature ($T > 1$)**:\n",
    "  - **Flatter Distribution**: Model explores more options; outputs are diverse.\n",
    "  - **Use Case**: When creativity is desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.5  # High randomness\n",
    "print(f\"Generated Names with temperature={temperature}:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.75  # Low randomness\n",
    "print(f\"Generated Names with temperature={temperature}:\\n\")\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1.0  # Default randomness\n",
    "prefix = \"Z\"  # Prefix to start the names with\n",
    "for _ in range(num_names):\n",
    "    name = generate_name(\n",
    "        model=model,\n",
    "        prefix=prefix,\n",
    "        tokenizer=tokenizer,\n",
    "        start_token=start_token,\n",
    "        end_token=end_token,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        device=device\n",
    "    )\n",
    "    print(name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vision)",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
